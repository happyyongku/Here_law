{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bc81ab-a16f-4ab0-b6b1-b9cbf1e25ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU 조절\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "# os.environ[\"GRADIO_SHARE\"]=\"3\"\n",
    "# os.environ[\"WORLD_SIZE\"] = \"3\"\n",
    "# os.getcwd()\n",
    "import torch\n",
    "torch.cuda.device_count() #must be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3cf0e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL_NAME = \"meetkai/functionary-small-v3.2\"\n",
    "MAX_SEQ_LENGTH = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "DTYPE = None #Auto\n",
    "LOAD_IN_4BIT = False\n",
    "LORA_R = 64 # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "LORA_ALPHA = 64\n",
    "LORA_RANDOM_STATE = 148177255\n",
    "EXCEL_PATH = '~/S11P21B109/backend/fastapi_gpu/finetuning/law_data.xlsx'\n",
    "TRAINER_BATCH=16\n",
    "TRAINER_ACCUMULATION=4\n",
    "TRAINER_EPOCH=2\n",
    "TRAINER_LR = 1e-4\n",
    "SAVE_SETTING = \"merged_16bit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953d548d-4191-43db-bfd9-473edfa28aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "# padding 없는 건 정상\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = BASE_MODEL_NAME,\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    dtype = DTYPE,\n",
    "    load_in_4bit = LOAD_IN_4BIT,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9a0dfe-7cb4-43a8-827a-b88cef3eeb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = LORA_R, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = LORA_ALPHA,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = LORA_RANDOM_STATE,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1999ec4-8308-47af-9ff3-222d1163160e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Step 1: Load the XLSX file into a pandas DataFrame\n",
    "df = pd.read_excel(EXCEL_PATH)  \n",
    "\n",
    "df = df.rename(columns={\n",
    "    'Prompt': 'instruction',\n",
    "    'User': 'input',\n",
    "    'Assistant': 'output'\n",
    "})\n",
    "df = df[['instruction', 'input', 'output']]\n",
    "\n",
    "# Convert columns to 'string' dtype\n",
    "df['instruction'] = df['instruction'].astype('string')\n",
    "df['input'] = df['input'].astype('string')\n",
    "df['output'] = df['output'].astype('string')\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "data_dict = df.to_dict('records')\n",
    "\n",
    "# Step 3: Convert the DataFrame into a Hugging Face Dataset\n",
    "dataset = Dataset.from_list(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60b42d23-485f-4749-8461-9dc9be50a067",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPECIAL TOKENS FOR LLAMA 3.1\n",
    "ROLE_SYSTEM_TOKEN = \"system\"\n",
    "ROLE_USER_TOKEN= \"user\"\n",
    "ROLE_ASSISTANT_TOKEN = \"assistant\"\n",
    "SH_TOKEN = \"<|start_header_id|>\"\n",
    "EH_TOKEN = \"<|end_header_id|>\"\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "#for prompts\n",
    "DOUBLE_NEWLINE = '\\n\\n'\n",
    "\n",
    "#Meekai/functionary token\n",
    "\n",
    "FUNCTIONARY_FUNCTION_SEPARATOR = \">>>\"\n",
    "FUNCTIONARY_PLAIN_TOKEN = 'all\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f7482a-37bf-48ba-b720-c9a89f0114e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DataCollatorForCompletionOnlyLM #for completion\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs       = examples[\"input\"]\n",
    "    outputs      = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs): #double newline 이미 있음\n",
    "        text = SH_TOKEN + ROLE_SYSTEM_TOKEN + EH_TOKEN + instruction + EOS_TOKEN\\\n",
    "        + SH_TOKEN + ROLE_USER_TOKEN + EH_TOKEN + DOUBLE_NEWLINE + input + EOS_TOKEN\\\n",
    "        + SH_TOKEN + ROLE_ASSISTANT_TOKEN + EH_TOKEN + DOUBLE_NEWLINE + FUNCTIONARY_FUNCTION_SEPARATOR + FUNCTIONARY_PLAIN_TOKEN +output + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = dataset.map(formatting_prompts_func, batched = True,)\n",
    "dataset_text = dataset.remove_columns([col for col in dataset.column_names if col != \"text\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f81601c-5215-41c0-a378-1fea9a710b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, concatenate_datasets, Dataset\n",
    "\n",
    "ds = load_dataset(\"openai/MMMLU\", \"by_language\")\n",
    "\n",
    "def formatting_prompts_func_qa(examples):\n",
    "    AT = {\n",
    "        \"A\" : \"1\",\n",
    "        \"B\" : \"2\",    \n",
    "        \"C\" : \"3\",\n",
    "        \"D\" : \"4\",\n",
    "    }\n",
    "    qa_instruction = \"You are a student AI studying to become a helpful assistant. you will be given a korean question. solve it. answer choice number only.\"    \n",
    "    questions = examples[\"Question\"]\n",
    "    aas = examples[\"A\"]\n",
    "    abs = examples[\"B\"]\n",
    "    acs = examples[\"C\"]\n",
    "    ads = examples[\"D\"]\n",
    "    answers = examples[\"Answer\"]\n",
    "    texts = []\n",
    "    for question, aa, ab, ac, ad, answer in zip(questions, aas, abs, acs, ads, answers):\n",
    "        text = SH_TOKEN + ROLE_SYSTEM_TOKEN + EH_TOKEN + DOUBLE_NEWLINE +qa_instruction + EOS_TOKEN\\\n",
    "        + SH_TOKEN + ROLE_USER_TOKEN + EH_TOKEN + DOUBLE_NEWLINE + question + \"\\n\" + f\"1:{aa}\\n2:{ab}\\n3:{ac}\\n4:{ad}\" + EOS_TOKEN\\\n",
    "        + SH_TOKEN + ROLE_ASSISTANT_TOKEN + EH_TOKEN + DOUBLE_NEWLINE + FUNCTIONARY_FUNCTION_SEPARATOR + FUNCTIONARY_PLAIN_TOKEN + AT[answer] + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return { \"text\" : texts, }\n",
    "\n",
    "dataset_qa = ds[\"KO_KR\"].map(formatting_prompts_func_qa, batched = True,)\n",
    "dataset_qa_text = dataset_qa.remove_columns([col for col in dataset_qa.column_names if col != \"text\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e5dde87-c44b-4c20-a27c-5a5c5735ab21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_dataset = concatenate_datasets([dataset_qa_text, dataset_text])\n",
    "merged_dataset = merged_dataset.shuffle(seed=LORA_RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dcd1309-68ca-4f5d-b03c-b9ae507e38dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset collator\n",
    "collator = DataCollatorForCompletionOnlyLM(SH_TOKEN + ROLE_ASSISTANT_TOKEN + EH_TOKEN, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51561277-3f98-40d9-9e46-1d0a6b63085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset[7000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6a941b-d88a-471d-bf01-deae6061a4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = merged_dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = MAX_SEQ_LENGTH,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = TRAINER_BATCH,\n",
    "        gradient_accumulation_steps = TRAINER_ACCUMULATION,\n",
    "        warmup_steps = 5,\n",
    "        num_train_epochs = TRAINER_EPOCH, # Set this for 1 full training run.\n",
    "        learning_rate = TRAINER_LR,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = LORA_RANDOM_STATE,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    "    data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95d7ca6-0a83-4177-aeda-f016e12425eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5601d18c-7124-49a6-8e24-05359d07ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcb4b7b-bffe-4e3c-8795-0190ffa28639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장. https://github.com/unslothai/unsloth/wiki 의 Manually saving to GGUF 참조\n",
    "model.save_pretrained_merged(\"merged_model\", tokenizer, save_method = SAVE_SETTING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d04946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llama cpp 를 실행하는 conda enviroment로 실행\n",
    "# git clone --recursive https://github.com/ggerganov/llama.cpp\n",
    "# make clean -C llama.cpp\n",
    "# make all -j -C llama.cpp\n",
    "# pip install gguf protobuf\n",
    "\n",
    "# python llama.cpp/convert_hf_to_gguf.py merged_model --outfile converted.gguf --outtype f16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6444378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Don't go more\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6fd4d7-94ae-4c28-b4d8-d282170ed284",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Model output check\n",
    "# alpaca_prompt = Copied from above\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "from transformers import TextStreamer\n",
    "\n",
    "for datadict in dataset:\n",
    "    inputs = tokenizer(\n",
    "        [\"<|start_header_id|>system<|end_header_id|>\" + datadict[\"instruction\"] + EOS_TOKEN\\\n",
    "        + \"<|start_header_id|>user<|end_header_id|>\" + datadict[\"input\"] + EOS_TOKEN\\\n",
    "        + \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "        ], return_tensors = \"pt\").to(\"cuda\")\n",
    "    text_streamer = TextStreamer(tokenizer)\n",
    "    _ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911e0ba6-0722-45b8-bbbd-87c49b0d8319",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
